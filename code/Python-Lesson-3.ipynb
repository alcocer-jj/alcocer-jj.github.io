{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1>\n",
    "Python for Social Science Workshop - Lesson 3\n",
    "</h1>\n",
    "</div>\n",
    "<br />\n",
    "<div align=\"center\">\n",
    "<h3>\n",
    "Jose J Alcocer\n",
    "</h3>\n",
    "</div>\n",
    "<br />\n",
    "<div align=\"center\">\n",
    "<h4>\n",
    "April 18, 2023\n",
    "</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate and Multivariate Regressions <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "This lesson will be covering applications of regressions using multiple datasets. The first part of this lesson will give a brief summary of the linear regression estimator to provide a refresher.\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "## 1.0 Summary of Linear Regression Estimator <br> <br>\n",
    "\n",
    "### What is Linear Regression? <br>\n",
    "\n",
    "Linear Regression is a statistical method used to capture linear relationships between two (or more) variables. <br>\n",
    "\n",
    "The model consists of: <br>\n",
    "\n",
    "* A dependent or outcome variable (**Y**) that denotes what we want to explain;\n",
    "* An independent or explanatory variable(s) (**X**) that denotes what we think explains a change in our outcome variable <br>\n",
    "\n",
    "An example of this can be the effect of education (**X**) on income (**Y**). <br> <br>\n",
    "\n",
    "### Choosing the Best Line <br>\n",
    "\n",
    "We strive to find the best fitting line as it allows us to:\n",
    "\n",
    "* Make predictions using new explanatory observations\n",
    "* Describe important relationships between explanatory and outcome variables\n",
    "* Make causal effect estimates (so long as our explanatory variable is randomly assigned) <br>\n",
    "\n",
    "The equation that gets us the best fitting line is: <br>\n",
    "\n",
    "$$ \\( Y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i} \\) $$ <br>\n",
    "\n",
    "Where: <br>\n",
    "\n",
    "* The $\\beta_{0}$ is the intercept or constant term; this tells us what our Y is when our X = 0, <br>\n",
    "* The $\\beta_{1}$ is the slope coefficient; this tells us how much Y is affected by X, and <br>\n",
    "* The $\\varepsilon$ is the error term; this represents all that is unobserved in this equation <br> <br>\n",
    "\n",
    "### How Do We Get the Best Line? <br>\n",
    "\n",
    "With so many possible lines, how exactly do we select the best one? <br>\n",
    "\n",
    "We select the combination of intercept $(\\(\\beta_{0}\\))$ and coefficient $(\\(\\beta_{1}\\))$ values from our line equation ($\\( Y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i} \\)$) that minimizes our sum of squared residuals (SSR). <br>\n",
    "\n",
    "The **sum of squared residuals** is non-other than the difference between our actual outcomes $(\\(Y_{i}\\))$ and our predicted outcomes $(\\(\\hat{Y_{i}}\\))$ generated by an iteration of a line equation being tested. <br>\n",
    "\n",
    "$$ SSR =  \\( \\sum_{i=1}^{n}(Y_{i} - \\hat{Y_{i}})^2 \\) $$ <br>\n",
    "\n",
    "One doesn't have to worry about how this formula is derived to obtain the ideal values, as we have statistical software help us; but, if interested then this [link](https://online.stat.psu.edu/stat501/lesson/1/1.2) shows how it is done. In addition, this [link](https://online.stat.psu.edu/stat501/lesson/1) provides an excellent review/introduction to regressions for those who may need it.<br> <br>\n",
    "\n",
    "Let's apply this estimator in Python using a made-up example to start off and then move onto to real-world datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setting Up the Environment <br>\n",
    "\n",
    "Let's download the library packages we will be using for this lesson. The package that will allow us to run regressions in Python is the `statsmodels` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import the first dataset we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "df = pd.read_csv('Salary_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Univariate Regression <br>\n",
    "\n",
    "The imported dataset reports year of experience at a certain job, their salary, and if the person is a woman. Suppose we want to run a regression where we are interested in understanding how years of work experience can affect one's salary. We would have to regress salary on years of experience. Before doing that exactly, let's begin by visualizing our data relationship. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize data by using the seaborn function `sns.regplot` and adding additional aesthetics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set theme\n",
    "sns.set_style('ticks')\n",
    "plot = sns.regplot(x='YearsExperience', y='Salary', data= df, color='black')\n",
    "# Adding title and labels\n",
    "plot.set_title('Figure 1', fontdict={'size': 18, 'weight': 'normal'})\n",
    "plot.set_xlabel('Years of Work Experience', fontdict={'size': 12})\n",
    "plot.set_ylabel('Salary (Dollars)', fontdict={'size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial visualization of the data shows that there may be a linear relationship between both years of work experience and salary. We can also see that our data appears homoskedastic (_there is no apparent variance or sparsity in the data observations_). Let's continue by fitting the model and running our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the regression\n",
    "model = smf.ols('Salary ~ YearsExperience', data=df).fit()\n",
    "\n",
    "# Presenting a table summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model.summary()` commands presents us with a nicely formatted table displaying a lot of information. Let's dissect it. <br> <br>\n",
    "\n",
    "### 2.1 Interpreting the Results\n",
    "\n",
    "#### 2.1.1 Top Section of Regresion Results\n",
    "\n",
    "The top section of the regression summary contains data on what was done and what was calculated in the regression model. <br>\n",
    "\n",
    "Starting with the left column of information, the summary tells us what our outcome variable of interest was (_in this case it was the salary of each individual_), followed by the model and method that was used to get the results (_in this case, it was an ordinary least squares regression that obtained the intercept and coefficient results by minimizing the sum of squared residuals_). <br>\n",
    "\n",
    "It then displays system information (the time and date that the regression was done) and it ends with some information about the dataset that it used to calculate the results (_the total number of observations in the dataset was 30 and the 'df residuals' is simply the total number of observations of the dataset subtracted by the number of variables being estimated_). <br>\n",
    "\n",
    "The last input on the left column displays what kind of covariance measure the model accounted for. Here, we have 'nonrobust', which is the default type of covariance measure that does not account for any Heteroskedasticity or Heteroskedasticity-Autocorrelation present in the data. We will run a different kind of regression that does account for these two later. <br>\n",
    "\n",
    "The right column of the top section continues to display data related to the regression model built. <br>\n",
    "\n",
    "Starting with the first two measures, the R-squared measures how well the line we found fits the data and explains the relationship. The R-squared gives us the percentage amount of variation explained by our regression line created. It uses the Sum of Squared Residuals (SSR) we calculated earlier to obtain this value. It does by dividing that SSR by the Total Sum of Sqaures (TSS), and it is denoted with the following equation: <br>\n",
    "\n",
    "$$ R^2 = 1 - \\frac{SSR}{TSS} $$ <br>\n",
    "\n",
    "If you are interested in knowing how the TSS is calculated, this three-minute [YouTube video](https://www.youtube.com/watch?v=A6OZMqdJHcc) explains it quite well. <br>\n",
    "\n",
    "Because our R-Squared metric ranges from 0 to 1, we can interpret a value closer to 1 as a higher value that explains the relationship between the data. A value closer to 0 would suggest that our regression does not explain the relationship between the variables. <br>\n",
    "\n",
    "The adjusted R-Squared is a modified version of the R-Squared that takes into account additional predictors that may be present in the model. Here, because we only have one independent variable, our Adjusted R-Squared does not vary by much. If you are interested in re-familiarizing yourself with these two concepts, the following two links are good resources to do so. <br>\n",
    "\n",
    "* [Towards Data Science article](https://towardsdatascience.com/r-squared-vs-adjusted-r-squared-simplified-543993e69558)\n",
    "* [Statology article](https://www.statology.org/adjusted-r-squared-interpretation/) <br>\n",
    "\n",
    "The F-Statistic measures the ratio of the mean squares treatment over the means squares error. It essentially measures the variation between sample means relative to the variation found within the samples used in the regression. Having a large F-Statistic value would suggest that there is evidence of a difference between the group means (_you can reject the null hypothesis of no effect_) <br>\n",
    "\n",
    "The Probability (F-Statistic) is none other than our collective p-value for our regression results. Per conventional standards, if our Prob. F-Statistic is lower than 0.05, then we can reject the null hypothesis of no effect (_meaning our results are statistically significant_). <br>\n",
    "\n",
    "We will ignore the last three measures as we do not need to understand these measures for our purposes. <br> <br>\n",
    "\n",
    "#### 2.1.2 Middle Section of Regression Results <br>\n",
    "\n",
    "The midsection of the regression summary is the most important, as this section displays our intercept and coefficient details. Let's interpret what we are seeing. <br>\n",
    "\n",
    "The first column displays all the variables that were inputted into the regression model. In this case, we only have our intercept and our years of experience variable. <br>\n",
    "\n",
    "The intercept is the value that our regression model equation produces if all independent variables were set to 0. <br>\n",
    "\n",
    "'YearsExperience' is our variable of interest and here we can see the beta coefficient's effect on salary. One would interpret this value as: an additional year of work experience is associated with an estimated increase of $9,450 dollars on one's salary, holding all else constant (that's a big jump). <br>\n",
    "\n",
    "The second column displays the standard errors (SEs) associated with the beta coefficients in our model. The standard error metric measures the sampling variation from our sample estimates (n) to the true population (N) that is unobserved. In other words, it is an indication of how well our sample estimates can be in relation to our beta coefficient(s). The higher the SE, the larger the variation from our sample to the true population pool. How does this relate to our estimated effects. If the SE of a coefficient were to be close to its estimated effect, then it would be assumed that the difference is close to none, making it indistinguishable from zero (non-significant effect). If the SE of a coefficient is not remotely close to our coefficient, then it can be suggested that our effect is rather large and therefore distinguishable from zero (significant effect). Because our coefficient is rather large from the SE, we can assume that there are distinguishable effects present. <br>\n",
    "\n",
    "The third column displays the test statistic, which is the coefficient divided by the standard error. <br>\n",
    "\n",
    "In order to get our p-value, one would need to take our test statistic and look at the t-distribution table with n-1 degrees of freedom to get that value. Luckily, the fourth column does that for us and presents the p-value of the beta coefficient estimate. Under arbitrary but conventional norms of significance, if our p-value is less than 0.05, then we can deem our coefficient as statistically significant. <br>\n",
    "\n",
    "The last column displays the beta coefficient's values within 95% of our data (or within 2 standard deviations of it). This is known as our confidence intervals. <br> <br>\n",
    "\n",
    "#### 2.1.3 Bottom Section of Regression Results <br>\n",
    "\n",
    "While the bottom section of the results contains multiple measures, the most important one to know is the Durbin-Watson measure. This measure tells us about the regression's amount of homoscedasticity, or how even are the distribution of errors in the data. Having a value between 1 and 2 would suggest that there is an acceptable amount of even distribution, or no presence of Heteroskedasticity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Multivariate Regression <br>\n",
    "\n",
    "Suppose we wanted to introduce an additional variable into our regression: the sex of the individual. We would then have to regress salary on years of experience and sex. Before doing that exactly, let's begin by visualizing our data relationship using one continuous and categorical variable. <br>\n",
    "\n",
    "While the previous example used `sns.regplot()` to plot our regression visualization, we will be using `sns.lmplot()` this time, as it will allow us to add a categorical component. There are two ways to plot a regression line, depending on how you want to convey the data. <br> <br>\n",
    "\n",
    "#### Method A: Both Lines In Same Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set theme\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Plot itself\n",
    "plot = sns.lmplot(x='YearsExperience', y='Salary', data= df, hue ='Sex', markers =['o', 'v'])\n",
    "\n",
    "# Plot title and axis labels\n",
    "plt.title('Figure 2')\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.xlabel(\"Years of Work Experience\")\n",
    "\n",
    "# Adjust x limits\n",
    "#plt.xlim(1,10)\n",
    "\n",
    "# Remove legend from outer layer to innner\n",
    "plt.tight_layout()\n",
    "\n",
    "# Set plot size for display\n",
    "plt.figure(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method B: Two Different Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set theme\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Plot itself\n",
    "plot = sns.lmplot(x='YearsExperience', y='Salary', data= df, col ='Sex')\n",
    "\n",
    "# Remove legend from outer layer to innner\n",
    "plt.tight_layout()\n",
    "\n",
    "# Set plot size for display\n",
    "plt.figure(figsize=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the regression. There are multiple ways to run a regression with a categorical variable as an additional independent variable. We will cover a few of those ways. <br> <br>\n",
    "\n",
    "#### Method 1: Straight Forward Plugging Variables In <br>\n",
    "\n",
    "Here, our coefficient tells us what the relationship between sex and salary is. Python automatically chooses one of the categories, and it gives us that relationship. Useful and easy, but not as modular if you were interested in having more control over what actually gets captured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the regression\n",
    "model = smf.ols('Salary ~ YearsExperience + Sex' , data=df).fit()\n",
    "\n",
    "# Presenting a table summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Specifying A Category <br>\n",
    "\n",
    "An alternative way to let Python know you are working with a categorical variable is by using the `C()` operator within the regression. As we can see, we are still having Python choose the variable for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the regression\n",
    "model = smf.ols('Salary ~ YearsExperience + C(Sex)' , data=df).fit()\n",
    "\n",
    "# Presenting a table summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 3: Manipulating the DataFrame to Create our Own Set of Dummies <br>\n",
    "\n",
    "A third way to let Python know you are working with a categorical variable is by creating a new variable that is a dummy of women. There are two ways to do this: (A) the long way, and (B) the short way. Let's cover both of them. <br> <br>\n",
    "\n",
    "#### 3A: The Intuitive But Long Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list\n",
    "list = []\n",
    "\n",
    "# Use a for-loop and if-else statement\n",
    "for x in df['Sex']:     # For every observation (row) in the variable, Sex\n",
    "    if x == 'Female':   # If the observation (row) in the variable, Sex, is equal to 'Female'\n",
    "        list.append(1)  # Add a 1 to nth observation in the list we created\n",
    "    else:\n",
    "        list.append(0)  # If it doesn't say \"Female\", add a 0 to that nth observation\n",
    "\n",
    "# Create a new variable and assign that list we created\n",
    "df['Female'] = list\n",
    "\n",
    "# Confirm that the variable is present\n",
    "print(df)\n",
    "\n",
    "# Running the regression\n",
    "model = smf.ols('Salary ~ YearsExperience + Female' , data=df).fit()\n",
    "\n",
    "# Presenting a table summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3B: The Easy Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset to clear it\n",
    "df = pd.read_csv('Salary_Data.csv')\n",
    "\n",
    "# Use pd.get_dummies to generate dummy variables out of Sex variable\n",
    "# You can use the argument 'drop_first=True' to drop whatever is the first dummy variable\n",
    "#   created\n",
    "Sex = pd.get_dummies(df['Sex'], drop_first=False)\n",
    "\n",
    "# Join new dataset with original one using 'df.join'\n",
    "# Unlike concat and merge, join command forces a join without any logic, this only works\n",
    "#   if your rows are aligned well\n",
    "df = df.join(Sex)\n",
    "\n",
    "# Running the regression\n",
    "model = smf.ols('Salary ~ YearsExperience + Female' , data=df).fit()\n",
    "\n",
    "# Presenting a table summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multivariate Regression with Interaction Terms <br>\n",
    "\n",
    "Suppose we wanted to know what are the effects of women and years of work experience on salary. Regressions tend to hold variables constant, so unless we tell python to interact both of them at the same time, we will not be able to observe the effect of both variables together. <br>\n",
    "\n",
    "We can do this by using the '*' operator within the regression function. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the regression\n",
    "model = smf.ols('Salary ~ YearsExperience * Female' , data=df).fit()\n",
    "\n",
    "# Presenting a table summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multivariate Regression with Robust Standard Errors <br>\n",
    "\n",
    "In addition, we can specify to Python that we want robust standard errors by inserting the \"cov_type = 'HC1'\" argument in the `.fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the regression\n",
    "model = smf.ols('Salary ~ YearsExperience * Female' , data=df).fit(cov_type='HC1')\n",
    "\n",
    "# Presenting a table summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Running Multiple Models and Presenting Them in A Table <br>\n",
    "\n",
    "Suppose we want to compare different regression models and present them in a more organized table. There are two methods that can allow us to do this. <br> <br>\n",
    "\n",
    "#### Method 1: Using Statsmodel Summary Col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import summary_col package\n",
    "from statsmodels.iolib.summary2 import summary_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run three separate regression models\n",
    "# Reg 1\n",
    "model1 = smf.ols('Salary ~ YearsExperience' , data=df).fit()\n",
    "# Reg 2\n",
    "model2 = smf.ols('Salary ~ YearsExperience + Female' , data=df).fit()\n",
    "# Reg 3\n",
    "model3 = smf.ols('Salary ~ YearsExperience * Female' , data=df).fit()\n",
    "\n",
    "dfoutput = summary_col([model1,model2,model3],stars=True, float_format='%0.2f',\n",
    "                       regressor_order=model1.params.index.tolist())\n",
    "print(dfoutput)\n",
    "\n",
    "# Use this to produce the table as LaTeX and copy/paste it to get actual table\n",
    "#dfoutput.as_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Using Stargazer Library (Originally R Package Now Adapted For Python) <br>\n",
    "\n",
    "The github repo for this package can be found [here](https://github.com/mwburke/stargazer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install package from CRAN\n",
    "# pip install stargazer\n",
    "\n",
    "# Import library package to workspace\n",
    "from stargazer.stargazer import Stargazer, LineLocation\n",
    "\n",
    "# Use previous models to create the table\n",
    "stargazer = Stargazer([model1,model2,model3] )\n",
    "\n",
    "# Specify significant digits\n",
    "stargazer.significant_digits(2)\n",
    "\n",
    "stargazer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we notice that our coefficients are a little wonky, as Python places them in alphabetical order. But, if we wanted them in a more traditional format, we can also specify it to the stargazer package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stargazer.covariate_order(['Intercept', 'YearsExperience', 'Female', 'YearsExperience:Female'])\n",
    "stargazer\n",
    "\n",
    "# The table we see below is HTML, if we want the code for that, all we need to execute is\n",
    "# stargazer.render_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can also render the table as LaTeX for easy exporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stargazer.render_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multivariate Regression Using Real Data <br> <br>\n",
    "\n",
    "#### Example 1 <br>\n",
    "\n",
    "Let's use a more real world example of data. The following data is from Zhou and Shaver's (2021) article, ['Reexamining the Effect of Refugees on Civil Conflict: A Global Subnational Analysis'](https://www.cambridge.org/core/journals/american-political-science-review/article/reexamining-the-effect-of-refugees-on-civil-conflict-a-global-subnational-analysis/9FB2BBEA2E2DC15560F367677C3D284E#article), and they seek to understand (among other things) whether the presence of refugees cause more conflict within states. While their paper conducts different analyses, the one we will be replicating is their robust regression of refugee presence on amount of Violent Events. The regression equation they use to estimate these effects is:  <br> <br>\n",
    "\n",
    "$$ ViolentEvents_{i,t} = \\beta_0 + \\beta_1 RefugeePresence_{i,t} + \\beta_2 RefugeePresenceinOtherProvinces_{i,t}  + $$\n",
    "$$ \\beta_3 LaggedViolentEvents_{i,t} + \\beta_4 NeighboringViolentEvents_{i,t} + $$\n",
    "$$ \\beta_5 LaggedPopulation_{i,t} + \\beta_6 LaggedGDP_{i,t} + \\beta_7 TerrainRuggedness_{i} + $$\n",
    "$$ \\beta_8 ProvinceSize_{i} + \\beta_9 BorderDistance_{i} + \\beta_{10} CapitalDistance_{i} +  $$\n",
    "$$ \\beta_{11} IDPPresence_{i,t} + CountryFixedEffects + YearFixedEffects + \\varepsilon_{i,t} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the data and view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('panel.full_GED_2020.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replication instructions tell us what variables were used for this regression analysis. Let's create a list object of those names and subset them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = ['attack','rtb','rtb.other','attack_1','attack_neighbors_sum', 'log_pop_1',\n",
    "            'gcp_ppp_1', 'STD', 'SQKM_ADMIN', 'log_bdist2', 'log_capdist', 'idpb',\n",
    "            'Country', 'year']\n",
    "df = df[var_list]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to the main analysis, let's discuss some useful commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gives us a set of statistics across all variables in the dataset\n",
    "# You can specify a few by indexing the df with those variables then placing describe after\n",
    "# exammple: df['attack'].describe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the country variable is categorical, so we can table it to see what countries are in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run correlations on variables we are interested in and provide aesthetic arguments to them. Here, we will just run a correlation on a few numeric variables in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the correlation in an object\n",
    "corr = df[['attack','rtb','log_pop_1',\n",
    "           'gcp_ppp_1']].corr(numeric_only=True) # this makes sure that it keeps numeric only\n",
    "\n",
    "# Aesthetic mapping\n",
    "corr.style.background_gradient(cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back to the main analysis. Let's run the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('attack ~ rtb + rtb.other + attack_1 + attack_neighbors_sum + log_pop_1 + gcp_ppp_1 + STD + SQKM_ADMIN + log_bdist2 + log_capdist + idpb + C(Country) + C(year)', data=df).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? It appears that Python is not recognizing the name 'rtb.other' in the regression. Let's change the name of the variable and rerun it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variable name\n",
    "df.rename(columns={'rtb.other':'rtb_other'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun regression\n",
    "model = smf.ols('attack ~ rtb + rtb_other + attack_1 + attack_neighbors_sum + log_pop_1 + gcp_ppp_1 + STD + SQKM_ADMIN + log_bdist2 + log_capdist + idpb + C(Country) + C(year)', data=df).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
